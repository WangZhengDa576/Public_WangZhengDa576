---
title: "03 XGBoost: Insight Edge"
output: html_document
date: "2025-07-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Load Packages

```{r}
# Install (only once)
# install.packages("xgboost")
# install.packages("dplyr")
# install.packages("MLmetrics")

library(xgboost)
library(dplyr)
library(MLmetrics)
```

## 2. Load and Prepare Training Data

```{r}
# Set working directory and read
setwd("C:/Users/Valerie/OneDrive - Singapore University of Technology and Design/00 YJ personal/00 Academics/00 Term 5 Mods/40.016 The Analytics Edge/00 1D Project/Submissions")
rm(list=ls())
train <- read.csv("train2024.csv")

# Drop unneeded columns and rename
smallertrain <- subset(train, select = -c(Case, No, Task, segment, year, miles, night, ppark, gender, age, educ, region, Urb, income))
smallesttrain <- subset(smallertrain, select = -c(milesa, nighta, agea, incomea))

colnames(smallesttrain)[1:91] <- paste0("X", 1:91)

# Convert one-hot target columns (Ch1–Ch4) to class index (0–3 for XGBoost)
smallesttrain$label <- apply(smallesttrain[, c("Ch1", "Ch2", "Ch3", "Ch4")], 1, function(row) which(row == 1) - 1)

# Train-test split (optional; useful for local validation)
set.seed(42)
train_index <- sample(1:nrow(smallesttrain), 0.7 * nrow(smallesttrain))
train_data <- smallesttrain[train_index, ]
valid_data <- smallesttrain[-train_index, ]

```

## 3. Create XGBoost DMatrix

```{r}
predictor_columns <- paste0("X", 1:91)

dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictor_columns]), label = train_data$label)
dvalid <- xgb.DMatrix(data = as.matrix(valid_data[, predictor_columns]), label = valid_data$label)
```

## 4. Cross-validation to Find Optimal Number of Rounds

``` {r}
params <- list(
  objective = "multi:softprob",
  num_class = 4,
  eval_metric = "mlogloss",
  eta = 0.05
)

# Cross-validation with early stopping
cv_result <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  early_stopping_rounds = 20,
  verbose = 1
)

# Extract best number of rounds
best_nrounds <- cv_result$best_iteration
```
## 5. Train Final XGBoost Model with Best nrounds

``` {r}
watchlist <- list(train = dtrain, eval = dvalid)

model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  watchlist = watchlist,
  early_stopping_rounds = 20,
  verbose = 1
)
```


# ---- Compute LogLoss on Validation Set ----

``` {r}
# Step 1: Get predicted probabilities for validation set
val_probs <- predict(model, newdata = dvalid)
val_probs_matrix <- matrix(val_probs, ncol = 4, byrow = TRUE)

# Step 2: One-hot encode true labels for validation set
true_labels <- valid_data$label
true_matrix <- matrix(0, nrow = length(true_labels), ncol = 4)
true_matrix[cbind(1:length(true_labels), true_labels + 1)] <- 1  # +1 for 1-indexing

# Step 3: Clip predicted probabilities to avoid log(0)
eps <- 1e-15
val_probs_matrix <- pmin(pmax(val_probs_matrix, eps), 1 - eps)

# Step 4: Calculate log loss
logloss <- -mean(rowSums(true_matrix * log(val_probs_matrix)))
cat(sprintf("LogLoss on Validation Set: %.5f\n", logloss))
```

## 6. Load and Prepare Test Data

```{r}
# Load test set
test_raw <- read.csv("test2024.csv")

# Drop unused columns
smallertest <- subset(test_raw, select = -c(Case, No, Task, segment, year, miles, night, ppark, gender, age, educ, region, Urb, income))
smallesttest <- subset(smallertest, select = -c(milesa, nighta, agea, incomea))

# Rename test feature columns
colnames(smallesttest)[1:91] <- paste0("X", 1:91)

# Create DMatrix for test
dtest <- xgb.DMatrix(data = as.matrix(smallesttest[, predictor_columns]))
```

## 7. Retrain Final Model on Full Data (using 313 rounds)

```{r}
# Combine training and validation data
train_data$choice <- NULL
full_data <- rbind(train_data, valid_data)

# Create DMatrix for full data
dtrain_full <- xgb.DMatrix(data = as.matrix(full_data[, predictor_columns]), label = full_data$label)

# Retrain using best nrounds from early stopping
final_model <- xgb.train(
  params = params,
  data = dtrain_full,
  nrounds = 313,
  verbose = 1
)
```

## 8. Create Submission File

```{r}
# Predict probabilities
probs <- predict(model, newdata = dtest)
probs_matrix <- matrix(probs, ncol = 4, byrow = TRUE)

# Name columns as Ch1, Ch2, Ch3, Ch4
colnames(probs_matrix) <- c("Ch1", "Ch2", "Ch3", "Ch4")

# Build final submission dataframe with soft probabilities
submission <- data.frame(
  No = 21566:(21566 + nrow(probs_matrix) - 1),
  probs_matrix
)

# Save to CSV 
write.csv(submission, "XGBoost_Model_Insight_Edge_Probabilities.csv", row.names = FALSE)
```